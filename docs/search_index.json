[
["index.html", "Getting started with Quantitative Research Methods and R Overview 0.1 Further sources", " Getting started with Quantitative Research Methods and R Lukas Wallrich 2020-03-03 Overview This guide supports the Core Quantitative Methods Course offered by the Goldsmiths’ Graduate School. It is a living document and currently not more than a rough draft, but will grow over time. It does not follow the order of sessions in the course, instead it is ordered in a way that might allow you to see connections and hopefully helps to look things up more easily. 0.1 Further sources This guide does not aim to be comprehensive, but just to provide sufficient orientation. There are many fantastic free online books that offer a more comprehensive treatment of many of the aspects discussed here, including the following: To be written "],
["simple-and-multiple-linear-regression.html", "Topic 1 Simple and multiple linear regression 1.1 Simple linear regression 1.2 Bivariate correlation 1.3 Multiple linear regression 1.4 Comparing means between conditions or groups", " Topic 1 Simple and multiple linear regression This document recaps how we assess relationships between variables. It is work in progress and will be updated as we explore some further tests and special cases in the weeks to come. library(tidyverse) 1.1 Simple linear regression For an introduction to simple linear regression and correlation, watch/rewatch this video When considering the relationship between two continuous variables, we should always start with a scatterplot - for example, of the relationship between social trust and life satisfaction at the national level in European countries (data from the European Social Survey 2014). #Data preparation ess &lt;- read_rds(url(&quot;http://empower-training.de/Gold/round7.RDS&quot;)) ess &lt;- ess %&gt;% mutate(soctrust = (ppltrst + pplfair + pplhlp)/3) nat_avgs &lt;- ess %&gt;% group_by(cntry) %&gt;% summarize(nat_soctrust = mean(soctrust, na.rm=T), nat_stflife = mean(stflife, na.rm=T)) ggplot(nat_avgs, aes(x=nat_soctrust, y=nat_stflife)) + geom_point() Figure 1.1: CAPTION THIS FIGURE!! The scatterplot can now help us to think about what kind of model would help us to explain or predict one variable based on the other. If a straight-line relationship seems reasonable, we can use a linear model. 1.1.1 Plotting a simple linear regression model Geometrically, a simple linear regression model is just a straight line through the scatterplot, fitted in a way that minimises the distances from the points to the line. It can be fitted with the geom_smooth(method=\"lm\") function. ggplot(nat_avgs, aes(x=nat_soctrust, y=nat_stflife)) + geom_point() + geom_smooth(method=&quot;lm&quot;, se=FALSE) Figure 1.2: CAPTION THIS FIGURE!! #se=FALSE hides confidence bands that are often just visual clutter For each value of one variable, this line now allows us to find the corresponding expected value of the other variable. 1.1.2 Mathematical representation of a simple linear regression model The idea of a simple linear regression model is to link one predictor variable, \\(x\\), to an outcome variable, \\(y\\). Their relationship can be expressed in a simple formula: \\[y=\\beta_{0} + \\beta_{1}*x\\] Here, \\(\\beta_{1}\\) is the most important parameter - it tells us, how much a change of 1 in the \\(x\\)-variable affects the \\(y\\)-variable. Geometrically, it is the slope of the regression line. \\(\\beta_{0}\\) is the intercept of that line, i.e. the value of \\(y\\) when \\(x\\) is 0 - since \\(x\\) can often not be zero, that parameter tends to be of little interest on its own. 1.1.3 Fitting a simple linear regression model in R Linear models are fitted in R using the lm() function. The model itself is specified as a formula with the notation y ~ x where the ~ means ‘is predicted by’ and the intercept is added automatically. lm(nat_stflife ~ nat_soctrust, data = nat_avgs) %&gt;% summary() ## ## Call: ## lm(formula = nat_stflife ~ nat_soctrust, data = nat_avgs) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.86854 -0.35172 0.00667 0.30745 0.85047 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.7767 0.7311 3.798 0.00122 ** ## nat_soctrust 0.8025 0.1347 5.957 9.84e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4633 on 19 degrees of freedom ## Multiple R-squared: 0.6513, Adjusted R-squared: 0.6329 ## F-statistic: 35.49 on 1 and 19 DF, p-value: 9.839e-06 In the summary, the first column of the coefficients table gives the estimates for \\(\\beta_{0}\\) (in the intercept row) and \\(\\beta_{1}\\) (in the nat_soctrust row). Thus in this case, the regression formula could be written as: \\[LifeSatisfaction = 2.76 + 0.80 * SocialTrust\\] Clearly this formula does not allow us to perfectly predict one variable from the other; we have already seen in the graph that not all points fall exactly on the line. The distances between each point and the line are the residuals - their distribution is described at the top of the summary()-function output. 1.1.3.1 Interpreting \\(R^2\\) The model output allows us to say how well the line fits by considering the \\(R^2\\) value. It is based on the sum of the squares of the deviations of each data point from either the mean (\\(SS_{total}\\)) or the model(\\(SS_{residual}\\)). If I was considering a model that predicted people’s height, I might have a person who is 1.6m tall. If the mean of the data was 1.8m, their squared total deviation would be \\(0.2*0.2 = 0.04\\). If the model then predicted their height at 1.55m, their squared residual deviation would be \\(-0.05*-0.05 = 0.025\\). Once this is summed up for all data points, \\(R^2\\) is calculated with the following formula: \\[R^2 = 1 - \\frac{SS_{residual}}{SS_{total}}\\] Given that the sum of squared residuals can never be less than 0 or more than the sum of the total residuals, \\(R^2\\) can only take up values between 0 and 1. It can be understood as the share of total variance explained by the model (or to link it to the formula: as the total variance minus the share not explained by the model). 1.1.3.2 Interpreting Pr(&gt;|t|), the p-value Each coefficient comes with an associated p-value in the summary that is shown at the end of the row. As indicated by the column title, it indicates the probability that the test statistic would be this large or larger if the null hypothesis was true and there was no association in the underlying population. As per usual, we would typically report coefficients with a p-value below .05 as statistically significant. The significance level for the intercept is almost never relevant and thus not reported (it simply tests whether the value of y would be different from 0 when x is zero, which is rarely of particular interest). 1.2 Bivariate correlation Simple linear regression can tell us whether two variables are linearly related to each other, whether this finding is statistically significant (i.e. unlikely to have arisen due to change), and how strong the relationship is in terms of the units of the two variables. Quite often, however, we would prefer a standardised measure of the strength of a relationship, and a quicker test. That is where correlation comes in. The Pearson’s correlation coefficient is the slope of the regression line when both variables are standardised, i.e. expressed in units of their standard deviations (as so-called Z-scores). It is again bounded, between -1 and 1, and as it is unit-free, it can give quick information as to which relationships matter more and which matter less. 1.2.1 Calculating the correlation coefficient Before calculating a correlation coefficient, you need to look at the scatterplot and make sure that a straight-line relationship looks reasonable. If that is the case, you can use the cor.test() function to calculate the correlation coefficient. cor.test(nat_avgs$nat_soctrust, nat_avgs$nat_stflife) #If you have missing data, use the na.rm = TRUE argument to have it excluded before the calculation ## ## Pearson&#39;s product-moment correlation ## ## data: nat_avgs$nat_soctrust and nat_avgs$nat_stflife ## t = 5.957, df = 19, p-value = 9.839e-06 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.5760041 0.9186640 ## sample estimates: ## cor ## 0.8070221 The estimated cor at the very bottom is the correlation coefficient, usually reported as r = .81. This shows that there is a strong positive relationship between the two variables. Check the p-value to see whether the relationship is statistically significant and the 95%-confidence interval to see how precise the estimate is likely to be. 1.2.2 Equivalence to linear regression Just to show that this is indeed equivalent to simple linear regression on the standardised variables, which can be calculated using the scale()-function. ggplot(nat_avgs, aes(x=scale(nat_soctrust), y=scale(nat_stflife))) + geom_point() + geom_smooth(method=&quot;lm&quot;, se=FALSE) Figure 1.3: CAPTION THIS FIGURE!! lm(scale(nat_stflife) ~ scale(nat_soctrust), data = nat_avgs) %&gt;% summary() ## ## Call: ## lm(formula = scale(nat_stflife) ~ scale(nat_soctrust), data = nat_avgs) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.13572 -0.45992 0.00872 0.40203 1.11209 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.524e-16 1.322e-01 0.000 1 ## scale(nat_soctrust) 8.070e-01 1.355e-01 5.957 9.84e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6059 on 19 degrees of freedom ## Multiple R-squared: 0.6513, Adjusted R-squared: 0.6329 ## F-statistic: 35.49 on 1 and 19 DF, p-value: 9.839e-06 Note that the regression coefficient for the scaled variable is now exactly identical to the correlation coefficient shown above. 1.3 Multiple linear regression Linear models can easily be extended to multiple predictor variables. Here I will focus on a few key points that particularly focus on linear models that include categorical predictor variables. You can watch/rewatch this video this video to take you through the conceptual process and some examples step-by-step. 1.3.1 Dummy coding revisited Multiple linear regression models often use categorical predictors. However, they need to be turned into numbers. This is done through automatic dummy coding, which creates variables coded as 0 and 1. Note that dummy coding always results in one fewer dummy variable than the number of levels of the categorical variable. For example, a gender variable with two levels (male/female) would be recoded into a single dummy variable female (0=no, 1=yes). Note that there is no equivalent variable male as that would be redundant. Given that the hypothetical gender variable here is defined as either male or female, not female necessarily implies male. The same applies to larger number of levels - see how three possible values for the department variabel can be recoded into two dummy variables: The level that is not explicitly mentioned anymore is the reference level. In a linear model, that is what all other effects are compared to, so it is important to keep in mind which that is. 1.3.2 Mathematical structure of multiple linear models Linear models are characterised by intercepts and slopes. Geometrically, intercepts shift a line or plane along an axis, while a slope changes its orientation in space. Conceptually, intercepts are like on/off switches, while slopes indicate what happens when variables are dialed up or down. For example, we might want to predict life expectancy based on a person’s gender and their level of physical activity. Then the model would be \\[LifeExpectancy=\\beta_{0} + \\beta_{1}*female + \\beta_{2}*activity\\] \\(\\beta_{0}\\) here would describe the notional life expectancy of males without any activity, \\(\\beta_{0}+\\beta_{1}\\) would describe that of females without any exercise - so both of these are intercepts, the \\(female\\) variable simply determines which intercept we choose. \\(\\beta_{2}\\) is now the effect of each unit of activity, i.e. the slope of the regression line. 1.3.3 Running multiple linear regression models in R In the lm()-function, it is very easy to keep on adding predictors to the formula by just using the +-symbol. For example, we can consider the UK General Election Results in 2019 and see what helps explain the vote share of the Conservatives. #Load and prep data constituencies &lt;- read_csv(url(&quot;http://empower-training.de/Gold/ConstituencyData2019.csv&quot;), col_types = &quot;_cfddddfffddddfffdfdddd&quot;) ## Warning: 168 parsing failures. ## row col expected actual file ## 47 ConShare2017 a double #N/A &lt;connection&gt; ## 534 ConShare2017 a double #N/A &lt;connection&gt; ## 534 ShareReligious a double #N/A &lt;connection&gt; ## 534 ShareChristian a double #N/A &lt;connection&gt; ## 534 DeprivationRankEngland a double #N/A &lt;connection&gt; ## ... ...................... ........ ...... ............ ## See problems(...) for more details. constituencies &lt;- constituencies %&gt;% filter(RegionName != &quot;Northern Ireland&quot;) %&gt;% mutate(nation = case_when(RegionName == &quot;Scotland&quot; ~ &quot;Scotland&quot;, RegionName == &quot;Wales&quot; ~ &quot;Wales&quot;, T ~ &quot;England&quot;)) %&gt;% filter(ConstituencyName != &quot;Chorley&quot;) #Simple linear regression model: lm(ElectionConShare ~ MedianAge, data = constituencies) %&gt;% summary() ## ## Call: ## lm(formula = ElectionConShare ~ MedianAge, data = constituencies) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.41131 -0.09130 0.01502 0.10007 0.30207 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.252020 0.040956 -6.153 1.35e-09 *** ## MedianAge 0.017104 0.001003 17.054 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1384 on 629 degrees of freedom ## Multiple R-squared: 0.3162, Adjusted R-squared: 0.3151 ## F-statistic: 290.8 on 1 and 629 DF, p-value: &lt; 2.2e-16 We now know that the Conservative vote share seems to be strongly related to the median age of the constituency, and can find and interpret the coefficient estimate (1.7 pct-points per year of median age), significance value and \\(R^2\\) as above. What happens when we add nation, to account for differences between Scotland, England and Wales? #Declare nation as factor and then set the reference level for categorical predictors constituencies$nation &lt;- factor(constituencies$nation) %&gt;% relevel(ref = &quot;England&quot;) lm(ElectionConShare ~ MedianAge + nation, data = constituencies) %&gt;% summary() ## ## Call: ## lm(formula = ElectionConShare ~ MedianAge + nation, data = constituencies) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.37287 -0.06969 0.00687 0.07799 0.27545 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.2777133 0.0337157 -8.237 1.03e-15 *** ## MedianAge 0.0185574 0.0008295 22.373 &lt; 2e-16 *** ## nationScotland -0.2527752 0.0156649 -16.136 &lt; 2e-16 *** ## nationWales -0.1493447 0.0187193 -7.978 7.08e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1138 on 627 degrees of freedom ## Multiple R-squared: 0.5392, Adjusted R-squared: 0.537 ## F-statistic: 244.6 on 3 and 627 DF, p-value: &lt; 2.2e-16 Now we have a more complex model. Based on the coefficient estimates, we would now estimate the vote share as follows \\[VoteShare=-0.27 + 0.018*age + -0.25*Scotland + -0.15*Wales\\] The figures for Scotland and Wales need to be compared to the unnamed reference level, i.e. to England - so we would expect a Scottish constituency to have a 25 percentage points lower Conservative vote share when keeping median age constant. Likewise, we now would expect an increase of the median age by 1 year to increase the Conservative vote share by 1.8 percentage points, keeping the effect of nation constant. 1.3.3.1 Significance testing and reporting in multiple regression models With more than one predictor, we have two questions: is the overall model significant, i.e. can we confidently believe that its estimates are better than if we just took the overall mean as our estimate for each constituency? That is shown in the last line of the summary. Here we would report that the regression model predicting the conservative vote share per constituency based on its median age and the nation it was located in was significant, with F(3, 627) = 244.6, p &lt; .001, and explained a substantial share of the variance, \\(R^2\\) = .54. does each predictor explain a significant share of unique variance? That is shown at the end of each line in the coefficients table, with many predictors it would usually be reported in a table. 1.4 Comparing means between conditions or groups Watch/rewatch this video for an introduction to testing for differences between two means and to the differences between repeated measures and independent samples designs. Watch/rewatch this video for an introduction to testing for differences between more than two means. 1.4.1 Repeated measures or independent samples The first question always needs to be whether each participant contributes one or several data points to the dependent variable that is compared. If they contribute only one, we have an independent samples or between-participants design; if they contribute several, the design is repeated measures or within participants. In the latter case, we need to account for the relationships between some of the measurements in our analysis. 1.4.2 Two independent means Going back to the European Social Survey 2014 data, we might be curious whether social trust differs between men and women in the UK. For that, we should always first calculate the descriptive statistics: ess &lt;- read_rds(url(&quot;http://empower-training.de/Gold/round7.RDS&quot;)) ess &lt;- ess %&gt;% mutate(soctrust = (ppltrst + pplfair + pplhlp)/3) essUK &lt;- ess %&gt;% filter(cntry==&quot;GB&quot;) essUK %&gt;% group_by(gndr) %&gt;% summarise(mean(soctrust, na.rm = TRUE)) ## # A tibble: 2 x 2 ## gndr `mean(soctrust, na.rm = TRUE)` ## &lt;fct&gt; &lt;dbl&gt; ## 1 Male 5.70 ## 2 Female 5.72 Those means look very close. Nevertheless, we might want to know how likely we would have been to see a difference this large under the null-hypothesis, i.e. if social trust did not differ between men and women. For that, we can run a t-test. t.test(soctrust ~ gndr, data=essUK, var.equal = TRUE) ## ## Two Sample t-test ## ## data: soctrust by gndr ## t = -0.2116, df = 2248, p-value = 0.8324 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.1601569 0.1289597 ## sample estimates: ## mean in group Male mean in group Female ## 5.702288 5.717886 This is just a special case of a linear model, so we could also use the lm() function: lm(soctrust ~ gndr, data=essUK) %&gt;% summary() ## ## Call: ## lm(formula = soctrust ~ gndr, data = essUK) ## ## Residuals: ## &lt;Labelled double&gt;: Most people can be trusted or you can&#39;t be too careful ## Min 1Q Median 3Q Max ## -5.7179 -1.0512 0.2821 1.2821 4.2977 ## ## Labels: ## value label ## 0 You can&#39;t be too careful ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 ## 6 6 ## 7 7 ## 8 8 ## 9 9 ## 10 Most people can be trusted ## NA(b) Refusal ## NA(c) Don&#39;t know ## NA(d) No answer ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.70229 0.05450 104.623 &lt;2e-16 *** ## gndrFemale 0.01560 0.07372 0.212 0.832 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.741 on 2248 degrees of freedom ## (14 observations deleted due to missingness) ## Multiple R-squared: 1.992e-05, Adjusted R-squared: -0.0004249 ## F-statistic: 0.04478 on 1 and 2248 DF, p-value: 0.8324 Both show, as expected, that the difference we observed could easily have been due to chance. If this is the only test you conduct, you would conventionally usually report the result of the t-test, saying: there was no significant difference in social trust between men and women, t(2248) = -0.21, p = .83. 1.4.3 Two dependent means In the ESS data, participants were asked how much they drank when they last drank during a weekday and during a weekend. Here the same participants provided two alcohol measures, so that these data points represent repeated measures. If we ignore that in the analysis, we violate a key assumption of linear models - namely the independence of observations. Therefore, we need to use a paired t-test. But as always, first descriptive statistics. essUK %&gt;% summarise(weekday = mean(alcwkdy, na.rm=T), weekend=mean(alcwknd, na.rm = T)) %&gt;% mutate(diff = weekend - weekday) ## # A tibble: 1 x 3 ## weekday weekend diff ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 36.3 62.4 26.1 So there seems to be a large difference in the average amount people drink during a single session on weekdays and weekends. Is the difference statistically significant? t.test(essUK$alcwknd, essUK$alcwkdy, paired = TRUE) ## ## Paired t-test ## ## data: essUK$alcwknd and essUK$alcwkdy ## t = 13.119, df = 1800, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 22.23332 30.04986 ## sample estimates: ## mean of the differences ## 26.14159 The paired t-test - as it says in the output - tests whether the mean of the differences between the two variables is significantly different from 0. We could also specify that condition directly: t.test(essUK$alcwknd - essUK$alcwkdy, mu = 0) ## ## One Sample t-test ## ## data: essUK$alcwknd - essUK$alcwkdy ## t = 13.119, df = 1800, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 22.23332 30.04986 ## sample estimates: ## mean of x ## 26.14159 1.4.4 More than two independent means We might be interested whether levels of life satisfaction differ between the European countries we could reach by ferry from the UK. First, let’s look at the descriptive statistics. essF &lt;- ess %&gt;% filter(cntry %in% c(&quot;FR&quot;, &quot;ES&quot;, &quot;IE&quot;, &quot;BE&quot;, &quot;NL&quot;)) essF %&gt;% group_by(cntry) %&gt;% summarise(mean(stflife, na.rm = T)) ## # A tibble: 5 x 2 ## cntry `mean(stflife, na.rm = T)` ## &lt;fct&gt; &lt;dbl&gt; ## 1 BE 7.45 ## 2 ES 6.96 ## 3 FR 6.39 ## 4 IE 6.94 ## 5 NL 7.60 There seem to be some differences - but are they statistically significant? Here we actually have two questions: do the countries together explain a significant share of the variance in life satisfaction? (omnibus test) are the levels of life satisfaction in any two countries significantly different from each other? (pairwise comparisons) 1.4.4.1 Omnibus test (ANOVA) #Set the reference level, so that we know what the coefficients mean essF$cntry &lt;- factor(essF$cntry) %&gt;% relevel(ref = &quot;FR&quot;) lm(stflife ~ cntry, data = essF) %&gt;% summary() ## ## Call: ## lm(formula = stflife ~ cntry, data = essF) ## ## Residuals: ## &lt;Labelled double&gt;: How satisfied with life as a whole ## Min 1Q Median 3Q Max ## -7.6017 -0.9646 0.3983 1.3983 3.6054 ## ## Labels: ## value label ## 0 Extremely dissatisfied ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 ## 6 6 ## 7 7 ## 8 8 ## 9 9 ## 10 Extremely satisfied ## NA(b) Refusal ## NA(c) Don&#39;t know ## NA(d) No answer ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.39456 0.04612 138.663 &lt;2e-16 *** ## cntryBE 1.05711 0.06651 15.893 &lt;2e-16 *** ## cntryES 0.57006 0.06512 8.753 &lt;2e-16 *** ## cntryIE 0.54832 0.06192 8.856 &lt;2e-16 *** ## cntryNL 1.20711 0.06516 18.526 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.016 on 9896 degrees of freedom ## (19 observations deleted due to missingness) ## Multiple R-squared: 0.04125, Adjusted R-squared: 0.04086 ## F-statistic: 106.4 on 4 and 9896 DF, p-value: &lt; 2.2e-16 Here we just want to see whether the country variable explains a significant share of the variance. That is shown by the last line, so that we would report: There was an overall effects of country on life satisfaction, F(4, 9869) = 106.4, p&lt;.001. If you are so inclined, note that this is identical to a one-way ANOVA. To see that, you can replace summary() by car::Anova() - however, this is only truly essential if you are a psychologist, most other disciplines prefer the plain linear models when they suffice. lm(stflife ~ cntry, data = essF) %&gt;% car::Anova() ## Anova Table (Type II tests) ## ## Response: stflife ## Sum Sq Df F value Pr(&gt;F) ## cntry 1730 4 106.45 &lt; 2.2e-16 *** ## Residuals 40218 9896 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 1.4.4.2 Pairwise comparisons Now that we know that some of the countries are different, we will want to locate the differences. That is where pairwise t-tests come in. pairwise.t.test(essF$stflife, essF$cntry, p.adjust.method = &quot;bonferroni&quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: essF$stflife and essF$cntry ## ## FR BE ES IE ## BE &lt; 2e-16 - - - ## ES &lt; 2e-16 2.4e-12 - - ## IE &lt; 2e-16 1.0e-14 1.00 - ## NL &lt; 2e-16 0.24 &lt; 2e-16 &lt; 2e-16 ## ## P value adjustment method: bonferroni This gives us p-values for all tests, that are adjusted for the fact that we are doing many (i.e. 10) comparisons and thus running a greater risk of getting a false positive. The bonferroni adjustment, selected here, multiplies each p-value by the number of comparisons, unless the resulting value would exceed 1 and thus be an impossible probability. Combining this with the descriptive statistics, we can say, for instance, that the people in the Netherlands and Belgium are more satisfied with life than those in any of the other countries, but that their satisfaction levels do not differ significantly from each other. 1.4.5 More than two means from repeated measures Here I will revert to the simulated example from the video linked above. In that, the effect of four conditions during studying on test score was tested, namely whether participants were exposed to instrumental music, vocal music, white noise or silence. Note that the data for repeated measures analaysis in R generally needs to be formatted in a way that each row shows one observation rather than multiple observations from one participant (“long” format). If you have data in a “wide” format, you can reshape it with the gather() function. To analyse whether there are differences between the conditions, as always, we start with descriptive statistics. noiseData %&gt;% group_by(condition) %&gt;% summarise(mean(score)) ## # A tibble: 4 x 2 ## condition `mean(score)` ## &lt;chr&gt; &lt;dbl&gt; ## 1 instrumental 11.6 ## 2 silence 13.0 ## 3 vocals 10.8 ## 4 whiteNoise 13.5 It looks like there are some differences, but to be able to judge statistical significance, we would again be interested in omnibus tests and then pairwise comparisons. 1.4.5.1 Omnibus test Testing whether the conditions make a difference is a little bit harder with repeated measures because the observations are not independent. Therefore, we need to run a model that takes into account the relationships between the observations taken from a single participant. This does not work with lm(); instead we need to use an additional package that allows for multi-level modeling where some observations are clustered together, lme4 is the most frequently used such package for this purpose. In the case of repeated measures, we were comparing our model with the group variable as a predictor implicitly to the model that only has the overall mean as a predictor (that is what the lm() F-test is doing). Here, the null model uses each participant’s own overall mean as the prediction for their performance in any one condition. We need to set up that null model explicitly and then compare it to the model that considers groups. #install.packages(lme4) library(lme4) ## Loading required package: Matrix ## ## Attaching package: &#39;Matrix&#39; ## The following objects are masked from &#39;package:tidyr&#39;: ## ## expand, pack, unpack ## Registered S3 methods overwritten by &#39;lme4&#39;: ## method from ## cooks.distance.influence.merMod car ## influence.merMod car ## dfbeta.influence.merMod car ## dfbetas.influence.merMod car #Set reference level explicitly noiseData$condition &lt;- noiseData$condition %&gt;% factor() %&gt;% relevel(&quot;silence&quot;) #Run null model - predicting only an individual intercept per participant model0 &lt;- lmer((score ~ (1 | participantID)), data = noiseData) #Run hypothesized model - adding the groups as a predictor model1 &lt;- lmer((score ~ condition + (1 | participantID)), data = noiseData) #Comparing the two models anova(model0, model1) ## refitting model(s) with ML (instead of REML) ## Data: noiseData ## Models: ## model0: score ~ (1 | participantID) ## model1: score ~ condition + (1 | participantID) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## model0 3 480.29 488.10 -237.14 474.29 ## model1 6 447.31 462.94 -217.66 435.31 38.978 3 1.754e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Here we can see that the hypothesised model showed a significantly better fit. This is tested with a \\(\\chi^2\\)-test, which we will look at further later in the course. 1.4.5.2 Pairwise comparisons Now that we know that there is a difference between some of the conditions, we will want to know which are different. For that, we can again run pairwise t-tests; we just need to specify that they are run on paired data by setting paired = TRUE. pairwise.t.test(noiseData$score, noiseData$condition, p.adj = &quot;bonferroni&quot;, paired=TRUE) ## ## Pairwise comparisons using paired t tests ## ## data: noiseData$score and noiseData$condition ## ## silence instrumental vocals ## instrumental 0.045 - - ## vocals 8.2e-06 0.423 - ## whiteNoise 1.000 0.001 2.0e-05 ## ## P value adjustment method: bonferroni This indicates that the scores of participants in the white noise and silence conditions were not significantly different from each other, while the other comparisons were significant. "],
["accessing-online-data-sources.html", "Topic 2 Accessing online data sources 2.1 World Bank data 2.2 Wikidata 2.3 Other data sources", " Topic 2 Accessing online data sources This section introduces some examples of R packages that allow you to access large secondary datasets. They are often a good way to understand wider trends, and thereby provide a high-level justification for doing research into a specific question. However, they can also be data sources for research projects in their own right. library(tidyverse) 2.1 World Bank data The World Bank offers a rich dataset with a particular focus on indicators relevant for the study of poverty, inequality and global development (in fact, much of gapminder is based on World Bank data). You can explore their data online on data.worldbank.com, or access it directly from R using the wbstats package. Here, I will explore the question whether life expectancy and literacy have increased in line with GDP in the BRICS countries (Brasil, Russia, India, China and South Africa, a group that has often been seen as representing emerging economies). #install.packages(&quot;wbstats&quot;) - needs to be run only once library(wbstats) #Download current list of indicators new_wb_cache &lt;- wbcache() #Search for indicators - you can best do this on data.worldbank.com and find the IndicatorID in the URL. The wbsearch() function often returns to many hits. #GDP per capita, purchasing power adjusted (to remove effect of exchange rates) wbsearch(&quot;gdp.*capita.*PPP&quot;, cache = new_wb_cache) ## indicatorID indicator ## 647 6.0.GDPpc_constant GDP per capita, PPP (constant 2011 international $) ## 10077 NY.GDP.PCAP.PP.CD GDP per capita, PPP (current international $) ## 10078 NY.GDP.PCAP.PP.KD GDP per capita, PPP (constant 2011 international $) ## 10079 NY.GDP.PCAP.PP.KD.87 GDP per capita, PPP (constant 1987 international $) ## 10080 NY.GDP.PCAP.PP.KD.ZG GDP per capita, PPP annual growth (%) Once we know the names of the indicators, we can download them. #Note: to get the country names, you can download all countries once and then check the names #wb_dat &lt;- wb(indicator = c(&quot;NY.GDP.PCAP.PP.KD&quot;, &quot;SI.POV.GINI&quot;), country = &quot;all&quot;) #wb_dat %&gt;% count(iso2c, country) %&gt;% view() wb_dat &lt;- wb(indicator = c(&quot;NY.GDP.PCAP.PP.KD&quot;, &quot;SP.DYN.LE00.IN&quot;, &quot;SE.TER.ENRR&quot;), country = c(&quot;IN&quot;, &quot;BR&quot;, &quot;CN&quot;, &quot;ZA&quot;, &quot;RU&quot;)) Now we have the data in a “long” format - with one combination of countries, indicators and years per row. That is a good layout for plotting, for other analyses you would need to reshape the data into a wide format where each indicator is in its own variable - look for the spread() function if you need that. A simple way of comparing the data is plotting the indicators side-by-side. One interesting take-away is that Brazil massively improved life expectancy and expanded education, even though GDP growth was rather modest, while South Africa stagnated in comparison. #Our GDP series only starts in 1990 - so it does not make sense to consider earlier life expectancy wb_datF &lt;- wb_dat %&gt;% filter(as.numeric(date)&gt;=1990) ggplot(wb_datF, aes(x=as.numeric(date), y=value, col=country)) + geom_point() + geom_line() + facet_wrap(.~indicator, scales = &quot;free&quot;, labeller = labeller(indicator = label_wrap_gen(25))) + #scales = &quot;free&quot; means that each indicator has its own y-axis, the labeller() function is needed for line breaks in the facet titles labs(title = &quot;Uneven progress in BRICS countries&quot;, subtitle = &quot;World Bank data&quot;, x = &quot;Year&quot;, y=&quot;&quot;, col = &quot;Country&quot;) Figure 2.1: CAPTION THIS FIGURE!! You can find a similar but slightly more detailed example for how to use the package here and very clear instructions in the the full README file of the package. 2.2 Wikidata Wikidata is where most data from Wikipedia and much else lives. So if there are Wikipedia articles on the topic you are interested in, you can likely find underlying data on Wikidata. For example, this might be used to quickly extract data on the gender of heads of government of many countries. Wikidata is based on data items that are connected by multiple relationships. So there will be an item for Germany, an item for Angela Merkel and a relationship for is the head of government of. Similarly, there is an item for country and a relationship for is an instance of that connects it to Germany. SPARQL queries are used to get the data - this article explains the logic quite well, but unless you want to spend a couple more weeks learning how to code, you can just take examples from Wikidata and adjust them as needed. For adjusting them, the online Wikidata Query Service works well, as it allows you to run the query again and again, until you get the data you need. I got curious about what share of the world’s population lives in countries with a female head of government, and how that varies by region. For that, I used the following code. #install.packages(&quot;WikidataQueryServiceR&quot;) library(WikidataQueryServiceR) headsOfGov &lt;- query_wikidata(&#39; SELECT ?country ?head ?gender ?countryLabel ?headLabel ?genderLabel ?continentLabel ?governmentLabel ?population WHERE { ?country wdt:P31 wd:Q6256 . ?country wdt:P6 ?head . ?head wdt:P21 ?gender . ?country wdt:P30 ?continent . ?country wdt:P1082 ?population . SERVICE wikibase:label { bd:serviceParam wikibase:language &quot;en&quot; } } ORDER BY ?countryLabel &#39;) ## 169 rows were returned by WDQS regional &lt;- headsOfGov %&gt;% group_by(continentLabel, genderLabel) %&gt;% summarise(pop = sum(population), n = n()) %&gt;% ungroup() world &lt;- data.frame(continentLabel = &quot;World&quot;, headsOfGov %&gt;% group_by(genderLabel) %&gt;% summarise(pop=sum(population), n=n()), stringsAsFactors = F ) world %&gt;% mutate(ShareOfCountries = n/sum(n)*100, ShareOfPopulation = pop/sum(pop)*100) %&gt;% filter(genderLabel == &quot;female&quot;) %&gt;% select(ShareOfCountries, ShareOfPopulation) %&gt;% round(1) %&gt;% kableExtra::kable(caption = &quot;&lt;b&gt;Women rule (%)&lt;/b&gt;&quot;) %&gt;% kableExtra::kable_styling(full_width = F) regionalAndWorld &lt;- rbind(regional, world) ggplot(regionalAndWorld, aes(x=continentLabel, y=pop, fill=genderLabel)) + geom_col(position=&quot;fill&quot;) + #Turns chart into bars rather than columns coord_flip() + #Show percentages rather than fractions on y-axis (now shown as x-axis) scale_y_continuous(labels=scales::percent) + labs(title=&quot;Only a small fraction of the world&#39;s population is ruled by women&quot;, subtitle=&quot;Source: WikiData, February 2020&quot;, x=&quot;&quot;, y=&quot;Share of population&quot;, fill=&quot;Head of government&quot;) Figure 2.2: CAPTION THIS FIGURE!! Table 2.1: Women rule (%) ShareOfCountries ShareOfPopulation 5.3 4.1 2.3 Other data sources Beyond the examples here, there are many other datasets to access. You might want to check out some of the following: The extensive list of political datasets compiled by Erik Gahner, with lots of current and historical data on anything from terrorism to government revenues and gender in politics. At the bottom, it also has a list of links to other lists of datasets. Eurostat offers a lot of statistics on all countries in Europe. In R, it can be accesses with the eurostat package; there is a good cheatsheet to help you get started The webpage asdfree.com/, with (sparse) instructions of how to access a wide range of online data sources, from very focused surveys such as the US National Longitudinal Study of Adolescent to Adult Health to international and very widely used datasets such as the World Values Survey. You can also use R to scrape data from pretty much any public webpage. This tutorial shows how to get data from IMDB, for instance. Finally, the essurvey package is the easiest way to get data from the European Social Survey into R. There is a good example for how to use it here "],
["two-more-pipes.html", "A Two more pipes", " A Two more pipes If you are clear on the pipe operator %&gt;% then two more pipes can help you save some typing and write more efficient code. However, neither are critical for this course. All of them require that you load the magrittr package explicitly - it is installed as part of the tidyverse, but not loaded. library(tidyverse) library(magrittr) #Example data constituencies &lt;- read_csv(url(&quot;http://empower-training.de/Gold/ConstituencyData2019.csv&quot;), col_types = &quot;_cfddddfffddddfffdfdddd&quot;) "],
["expanding-the-tidyverse-the-exposition-pipe.html", "B Expanding the tidyverse: the exposition pipe (%$%)", " B Expanding the tidyverse: the exposition pipe (%$%) Inside dplyr pipes, you do not need to use the $ to access variables within the dataframe. However, that does not work in some base-R functions such as cor.test(). For them, %$% can expose the variables in a dataframe temporarily (until the end of that command) so that they can be accessed as if they were separate variables in your environment. #Normal code cor.test(constituencies$MedianAge, constituencies$ElectionConShare) #With the exposition pipe constituencies %$% cor.test(MedianAge, ElectionConShare) ## ## Pearson&#39;s product-moment correlation ## ## data: constituencies$MedianAge and constituencies$ElectionConShare ## t = 16.454, df = 648, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.4862318 0.5949037 ## sample estimates: ## cor ## 0.542836 ## ## ## Pearson&#39;s product-moment correlation ## ## data: MedianAge and ElectionConShare ## t = 16.454, df = 648, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.4862318 0.5949037 ## sample estimates: ## cor ## 0.542836 Clearly in this case, the difference is slight but you might find it more readable as well. In other cases, this can spare you from having to save a filtered dataset or otherwise processed dataset into a new variable that you are only using once. constituencies %&gt;% filter(ContType == &quot;County&quot;) %$% cor.test(MedianAge, ElectionConShare) ## ## Pearson&#39;s product-moment correlation ## ## data: MedianAge and ElectionConShare ## t = 7.4961, df = 366, p-value = 5.003e-13 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.2727644 0.4502600 ## sample estimates: ## cor ## 0.3648222 "],
["a-somewhat-risky-time-saver-the-assignment-pipe.html", "C A somewhat risky time-saver: the assignment pipe (%&lt;&gt;%)", " C A somewhat risky time-saver: the assignment pipe (%&lt;&gt;%) Quite often, we want to edit an element in place, which leads to some repetition as you need to specify that you want to put it into the pipe and then save it back in place. #Normal code constituencies &lt;- constituencies %&gt;% filter(ConstituencyName != &quot;Chorley&quot;) #With assignment pipe constituencies %&lt;&gt;% filter(ConstituencyName != &quot;Chorley&quot;) The assignment pipe here takes constituencies, puts it into the pipeline, and takes the result at the end to save back into constituencies. It can be particularly pleasant when you want to apply a function to a single variable, but be careful not to use it accidentally - if you type %&lt;&gt;% instead of %&gt;% you will create bugs in your code that are not easy to spot. constituencies$ElectionWon %&lt;&gt;% relevel(ref=&quot;Lab&quot;) "]
]
